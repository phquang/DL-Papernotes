# Paper List
==========

## Iconic (Last Updated Nov 29, 2016)
- Building High-level Features Using Large Scale Unsupervised Learning [[arXiv]](https://arxiv.org/pdf/1112.6209.pdf)
- ImageNet Classification with Deep Convolutional Neural Networks [[NIPS]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- Dropout: A Simple Way to Prevent Neural Networks from Overfitting [[JMLR]](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
- Long Short-Term Memory [[CMU]](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)

## Generative Adversarial Networks (Last Updated Nov 29, 2016)
- Generative Adversarial Networks [[arXiv]](https://arxiv.org/pdf/1406.2661v1.pdf)
- Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks [[arXiv]](https://arxiv.org/pdf/1506.05751.pdf)
- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks [[arXiv]](https://arxiv.org/pdf/1511.06434v2.pdf)
- Context Encoders: Feature Learning by Inpainting [[arXiv]](https://arxiv.org/pdf/1604.07379v2.pdf)
- Coupled Generative Adversarial Networks [[arXiv]](https://arxiv.org/pdf/1606.07536.pdf)
- SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[arXiv]](https://arxiv.org/pdf/1609.05473v4.pdf)
- GANs for Sequence of Discrete Elements with the Gumbel-softmax Distribution [[arXiv]](https://arxiv.org/pdf/1611.04051v1.pdf)
- Adversarial Autoencoders [[arXiv]](https://arxiv.org/pdf/1511.05644.pdf)
- Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks [[arXiv]](https://arxiv.org/pdf/1511.06390v2.pdf)
- Adversarial Training Methods for Semo-Supervised Text Classification [[arXiv]](https://arxiv.org/pdf/1605.07725v2.pdf)

## Generative Models (Last Updated Nov 30, 2016)
- Pixel Recurrent Neural Network [[arXiv]](https://arxiv.org/pdf/1601.06759v3.pdf)

## Regularization (Last updated Nov 29, 2016)
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units [[arXiv]](https://arxiv.org/pdf/1504.00941v2.pdf)
- Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations [[arXiv]](https://arxiv.org/pdf/1606.01305v2.pdf)
- Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks [[NIPS]](https://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf)
- Semi-supervised Sequence Learning [[NIPS]](https://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf)
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units [[arXiv]](https://arxiv.org/pdf/1504.00941.pdf)
- Recurrent Neural Network Regularization [[arXiv]](https://arxiv.org/pdf/1409.2329.pdf)
- Regularizing RNNs by Stabilizing Activations [[arXiv]](https://arxiv.org/pdf/1511.08400v7.pdf)
- Recurrent Batch Normalization [[arXiv]](https://arxiv.org/pdf/1603.09025.pdf)
- Layer Normalization [[arXiv]](https://arxiv.org/pdf/1607.06450.pdf)
- Dropout Training as Adaptive Regularization [[NIPS]](https://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)
- Dropout Distillation [[JMLR]](http://jmlr.org/proceedings/papers/v48/bulo16.pdf)
- Regularizing RNNs by Stabilizing Activations [[arXiv]](https://arxiv.org/pdf/1511.08400v7.pdf)

## Architectures (Last Updated Nov 29, 2016)

### General Deep Nets
- Fitnets: Hints for Thin Deep Nets [[arXiv]](https://arxiv.org/pdf/1412.6550v4.pdf)
- Highway Networks [[arXiv]](https://arxiv.org/pdf/1505.00387.pdf)
- Deeply-Supervised Nets [[JMLR]](http://jmlr.org/proceedings/papers/v38/lee15a.pdf)

### Convolutional Neural Networks
- Visualizing and Comparing Convolutional Neural Networks [[arXiv]](https://arxiv.org/pdf/1412.6631v2.pdf)
- Deeply-Fused Nets [[arXiv]](https://arxiv.org/pdf/1605.07716v1.pdf)
- FractalNet: Ultra-Deep Neural Networks Without Residuals [[arXiv]](https://arxiv.org/pdf/1605.07648v2.pdf)
- Higher Order Recurrent Networks [[arXiv]](https://arxiv.org/pdf/1605.00064v1.pdf)
- Rethinking the Inception Architecture for Computer Vision [[arXiv]](https://arxiv.org/pdf/1512.00567v3.pdf)

### Recurrent Neural Networks
- How to Construct Deep Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1312.6026v5.pdf)
- Gated Recurrent Units [[arXiv]](https://arxiv.org/pdf/1406.1078v3.pdf), [[arXiv]](https://arxiv.org/pdf/1412.3555v1.pdf)
- Grid Long-Short Term Memory [[arXiv]](https://arxiv.org/pdf/1507.01526v3.pdf)
- Simplifying Long-Short term Memory Acoustic For Fast Training and Decoding [[Microsoft]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/lstm_simplification-1.pdf)
- Recurrent Highway Networks [[arXiv]](https://arxiv.org/pdf/1607.03474v3.pdf)
- Segmental Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1511.06018v2.pdf)
- Sequential Neural Models with Stochastic Layers [[arXiv]](https://arxiv.org/pdf/1605.07571.pdf)
- Learning Stochastic Recurrent Networks [[arXiv]](https://arxiv.org/pdf/1411.7610v3.pdf)
- Unitary Evolution Recurrent Neural Networks [[JMLR]](http://jmlr.org/proceedings/papers/v48/arjovsky16.pdf)
- Gated Feedback Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1502.02367.pdf)
- HyperNetworks [[arXiv]](https://arxiv.org/pdf/1609.09106v3.pdf)
- Memory Networks [[arXiv]](https://arxiv.org/pdf/1410.3916v11.pdf)
- Hierarchical Memory Networks [[arXiv]](https://arxiv.org/pdf/1605.07427v1.pdf)
- Quasi-Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1611.01576v2.pdf)
- Minimal Gated Unit for Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1603.09420.pdf)
- Hierarchical Multiscale Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1609.01704.pdf)
- LightRNN: Memory and Computation-Efficient Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1610.09893v1.pdf)

## DL Theory (Last Updated Nov 29, 2016)
- A Mathematical Motivation for Complex-valued Convolutional Networks [[arXiv]](https://arxiv.org/pdf/1503.03438v3.pdf)
- A Probabilistic Theory of Deep Learning [[arXiv]](https://arxiv.org/pdf/1504.00641v1.pdf)
- A Theoretically Grounded Application of Dropout in Recurrent Neural Networks [[arXiv]](https://arxiv.org/pdf/1512.05287v5.pdf)
- Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity [[arXiv]](https://arxiv.org/pdf/1602.05897v1.pdf)
- Learning the Number of Neurons in Deep Networks [[NIPS]](http://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks.pdf)
- A Theory of Generative ConvNet [[arXiv]](http://arxiv.org/pdf/1602.03264v3.pdf)
- The loss surface of Multilayer Nets [[arXiv]](https://arxiv.org/pdf/1412.0233v3.pdf)

## Training Algorithms (Last Updated Nov 29, 2016)
- Training Recurrent Networks Online Without Backtracking [[arXiv]](https://arxiv.org/pdf/1507.07680v2.pdf)
- Training Recurrent Neural Network by Diffusion [[arXiv]](https://arxiv.org/pdf/1601.04114v2.pdf)
- How (not) to Train Your Generative Model: Scheduled Sampling, Likelihood, Adversary? [[arXiv]](https://arxiv.org/pdf/1511.05101.pdf)

## Reinforcement Learning (Last Updated Nov 29, 2016)
- A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models [[arXiv]](https://arxiv.org/pdf/1611.03852v3.pdf)
- Neural Architecture Search with Reinforcement Learning [[arXiv]](https://arxiv.org/pdf/1611.01578v1.pdf)
- Reinforcement Learning with Unsupervised Auxiliary Tasks [[arXiv]](https://arxiv.org/pdf/1611.05397v1.pdf)

## Distillation and Bayesian Dark Knowledge
- Do deep Nets Really Need to be Deep? [[NIPS]](http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf)
- Distilling the Knowledge in a Neural Network [[arXiv]](http://arxiv.org/pdf/1503.02531v1.pdf)
- Bayesian Learning via Stochastic Gradient Langevin Dynamics [[ICML]](http://www.icml-2011.org/papers/398_icmlpaper.pdf)
- Bayesian Dark Knowledge [[NIPS]](https://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf)

## Others (Last Updated Nov 30, 2016)
- Categorical Reparameterization with Gumbel-Softmax [[arXiv]](https://arxiv.org/pdf/1611.01144v2.pdf)
- Growing Recursive Self-Improvers [[IDSIA]](http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf)
- Neural Programmer-Interpreters [[arXiv]](https://arxiv.org/pdf/1511.06279v4.pdf)

