# Paper List
==========

## Iconic (Last Updated Nov 29, 2016)
- Building High-level Features Using Large Scale Unsupervised Learning [[arXiv]](https://arxiv.org/abs/1112.6209)
- ImageNet Classification with Deep Convolutional Neural Networks [[NIPS]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-)
- Dropout: A Simple Way to Prevent Neural Networks from Overfitting [[JMLR]](http://www.jmlr.org/papers/v15/srivastava14a.html)
- Long Short-Term Memory [[CMU]](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)

## Generative Adversarial Networks (Last Updated Nov 29, 2016)
- Generative Adversarial Networks [[arXiv]](https://arxiv.org/abs/1406.2661)
- Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks [[arXiv]](https://arxiv.org/abs/1506.05751)
- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks [[arXiv]](https://arxiv.org/1511.06434)
- Context Encoders: Feature Learning by Inpainting [[arXiv]](https://arxiv.org/abs/1604.07379)
- Coupled Generative Adversarial Networks [[arXiv]](https://arxiv.org/abs/1606.07536)
- SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[arXiv]](https://arxiv.org/abs/1609.05473)
- GANs for Sequence of Discrete Elements with the Gumbel-softmax Distribution [[arXiv]](https://arxiv.org/abs/1611.04051)
- Adversarial Autoencoders [[arXiv]](https://arxiv.org/abs/1511.05644)
- Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks [[arXiv]](https://arxiv.org/abs/1511.06390)
- Adversarial Training Methods for Semo-Supervised Text Classification [[arXiv]](https://arxiv.org/abs/1605.07725)

## Generative Models (Last Updated Nov 30, 2016)
- Pixel Recurrent Neural Network [[arXiv]](https://arxiv.org/abs/1601.06759v3)
- Discrete Variational Autoencoder [[Temp]](http://openreview.net/pdf?id=ryMxXPFex)

## Regularization (Last updated Nov 29, 2016)
- A Simple Way to Initialize Recurrent Networks of Rectified Linear Units [[arXiv]](https://arxiv.org/abs/1504.00941)
- Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations [[arXiv]](https://arxiv.org/abs/1606.01305)
- Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1506.03099)
- Semi-supervised Sequence Learning [[NIPS]](https://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf)
- Recurrent Neural Network Regularization [[arXiv]](https://arxiv.org/abs/1409.2329)
- Regularizing RNNs by Stabilizing Activations [[arXiv]](https://arxiv.org/abs/1511.08400v7)
- Recurrent Batch Normalization [[arXiv]](https://arxiv.org/abs/1603.09025)
- Layer Normalization [[arXiv]](https://arxiv.org/abs/1607.06450)
- Dropout Training as Adaptive Regularization [[NIPS]](https://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)
- Dropout Distillation [[JMLR]](http://jmlr.org/proceedings/papers/v48/bulo16.pdf)
- Regularizing RNNs by Stabilizing Activations [[arXiv]](https://arxiv.org/pdf/1511.08400v7.pdf)
- Unsupervised Pretraining for Sequence to Sequence Learning [[Temp]](http://openreview.net/pdf?id=H1Gq5Q9el)
- Dropout with Expectation-Linear Regularization [[Temp]](http://openreview.net/pdf?id=rkGabzZgl)

## Architectures (Last Updated Nov 29, 2016)

### General Deep Nets
- Fitnets: Hints for Thin Deep Nets [[arXiv]](https://arxiv.org/abs/1412.6550)
- Highway Networks [[arXiv]](https://arxiv.org/abs/1505.00387)
- Deeply-Supervised Nets [[JMLR]](http://jmlr.org/proceedings/papers/v38/lee15a.pdf)

### Convolutional Neural Networks
- Visualizing and Comparing Convolutional Neural Networks [[arXiv]](https://arxiv.org/abs/1412.6631)
- Deeply-Fused Nets [[arXiv]](https://arxiv.org/abs/1605.07716)
- FractalNet: Ultra-Deep Neural Networks Without Residuals [[arXiv]](https://arxiv.org/abs/1605.07648)
- Higher Order Recurrent Networks [[arXiv]](https://arxiv.org/abs/1605.00064)
- Rethinking the Inception Architecture for Computer Vision [[arXiv]](https://arxiv.org/abs/1512.00567)

### Recurrent Neural Networks
- How to Construct Deep Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1312.6026)
- Gated Recurrent Units [[arXiv]](https://arxiv.org/pdf/1406.1078v3.pdf), [[arXiv]](https://arxiv.org/abs/1412.3555)
- Grid Long-Short Term Memory [[arXiv]](https://arxiv.org/abs/1507.01526)
- Simplifying Long-Short term Memory Acoustic For Fast Training and Decoding [[Microsoft]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/lstm_simplification-1.pdf)
- Recurrent Highway Networks [[arXiv]](https://arxiv.org/abs/1607.03474)
- Segmental Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1511.06018)
- Sequential Neural Models with Stochastic Layers [[arXiv]](https://arxiv.org/abs/1605.07571)
- Learning Stochastic Recurrent Networks [[arXiv]](https://arxiv.org/abs/1411.7610)
- Unitary Evolution Recurrent Neural Networks [[JMLR]](http://jmlr.org/proceedings/papers/v48/arjovsky16.pdf)
- Gated Feedback Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1502.02367)
- HyperNetworks [[arXiv]](https://arxiv.org/abs/1609.09106)
- Memory Networks [[arXiv]](https://arxiv.org/abs/1410.3916)
- Hierarchical Memory Networks [[arXiv]](https://arxiv.org/abs/1605.07427)
- Quasi-Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1611.01576)
- Minimal Gated Unit for Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1603.09420)
- Hierarchical Multiscale Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1609.01704)
- LightRNN: Memory and Computation-Efficient Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1610.09893)
- Hierarchical Multiscale Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1609.01704)
- Higher Order Recurrent Neural Networks [[Temp]](http://openreview.net/pdf?id=ByZvfijeg)

## DL Theory (Last Updated Nov 29, 2016)
- A Mathematical Motivation for Complex-valued Convolutional Networks [[arXiv]](https://arxiv.org/abs/1503.03438)
- A Probabilistic Theory of Deep Learning [[arXiv]](https://arxiv.org/abs/1504.00641)
- A Theoretically Grounded Application of Dropout in Recurrent Neural Networks [[arXiv]](https://arxiv.org/abs/1512.05287)
- Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity [[arXiv]](https://arxiv.org/abs/1602.05897)
- Learning the Number of Neurons in Deep Networks [[NIPS]](http://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks.pdf)
- A Theory of Generative ConvNet [[arXiv]](http://arxiv.org/abs/1602.03264)
- The loss surface of Multilayer Nets [[arXiv]](https://arxiv.org/abs/1412.0233)

## Training Algorithms (Last Updated Nov 29, 2016)
- Training Recurrent Networks Online Without Backtracking [[arXiv]](https://arxiv.org/abs/1507.07680)
- Training Recurrent Neural Network by Diffusion [[arXiv]](https://arxiv.org/abs/1601.04114)
- How (not) to Train Your Generative Model: Scheduled Sampling, Likelihood, Adversary? [[arXiv]](https://arxiv.org/abs/1511.05101)

## Reinforcement Learning (Last Updated Nov 29, 2016)
- A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models [[arXiv]](https://arxiv.org/abs/1611.03852)
- Neural Architecture Search with Reinforcement Learning [[arXiv]](https://arxiv.org/abs/1611.01578)
- Reinforcement Learning with Unsupervised Auxiliary Tasks [[arXiv]](https://arxiv.org/abs/1611.05397)

## Distillation and Bayesian Dark Knowledge
- Do deep Nets Really Need to be Deep? [[NIPS]](http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf)
- Distilling the Knowledge in a Neural Network [[arXiv]](http://arxiv.org/abs/1503.02531)
- Bayesian Learning via Stochastic Gradient Langevin Dynamics [[ICML]](http://www.icml-2011.org/papers/398_icmlpaper.pdf)
- Bayesian Dark Knowledge [[NIPS]](https://papers.nips.cc/paper/5965-bayesian-dark-knowledge.pdf)

## Others (Last Updated Nov 30, 2016)
- Categorical Reparameterization with Gumbel-Softmax [[arXiv]](https://arxiv.org/abs/1611.01144)
- Growing Recursive Self-Improvers [[IDSIA]](http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf)
- Neural Programmer-Interpreters [[arXiv]](https://arxiv.org/abs/1511.06279)
- Nonparametric Neural Networks [[Temp]](https://openreview.net/pdf?id=BJK3Xasel)
- On Orthogonality and Learning Recurrent Networks with Long Term Dependencies [[Temp]](https://openreview.net/pdf?id=HkuVu3ige)

